{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "> Some of the explanations in this section are inspired by chapter 3 in the book _\"Python Machine Learning\"_ by Sebastian Raschka\n",
    "\n",
    "First, let's introduce what Logistic Regression is. \n",
    "\n",
    "[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) is a classification model that is very easy to implement and performs very well on linearly separable classes. It is one of the most widely used\n",
    "algorithms for classification in industry too, which makes it attractive to play with.\n",
    "\n",
    "_Very_ simplistically explained, Logistic Regression works as follows:\n",
    "\n",
    "![Logistic Regression](./images/logistic_regression.png)\n",
    "\n",
    "First we will define the input for our algorithm. The imput will be each sample in whatever dataset we are working with. Each sample will consist of several features. For example, if we're working with housing price prediction, the features for each sample could be the size of the house, number of rooms, etc. We'll call the input vector **X**.\n",
    "\n",
    "For the algorythm to learn, we need to define variables that we can adjust accordingly to what we want to predict. We will create a vector of _weights_ (**W**) that the model will adjust in order to predict more accurately. The process of adjusting those weights is what we call **learning**.\n",
    "\n",
    "For every input sample, we will perform a dot product of the features by the weights **XW**. This product is sometimes referred as _net input_. This will give us a real number. Since in this particular problem we want to _classify_ (positive/negative), we need squash this number in the range [0, 1]. This will give us the _probability_ of a positive event. A function that does precisely that is called **sigmoid**. The sigmoid function looks like this:\n",
    "\n",
    "![sigmoid](./images/sigmoid.svg)\n",
    "\n",
    "What sigmoid is doing is basically transforming big inputs into a value close to 1, and small inputs into a value close to 0. This is exactly what we want. \n",
    "\n",
    "We will do this for every sample in our training set and compute the errors. To calculate the error we only need to compare our prediction with the true label for each sample. We will sum the square errors of all the samples to get a global prediction error. This will be our **cost function**.\n",
    "\n",
    "A cost function is then something we want to minimize. **Gradient descent** is a method for finding the minimum of a function of multiple variables, such like the one we're dealing with here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gradient descent\n",
    "\n",
    "> Watch [this](https://www.youtube.com/watch?v=IHZwWFHWa-w) video for a visual introduction to Gradient Descent\n",
    "\n",
    "Once all our training samples have been computed and the error calculated with our cost function, we need to _minimize_ that cost function. A method for doing that is gradient descent. There are many [articles](https://en.wikipedia.org/wiki/Gradient_descent) that contain detailed explanations _and_ implementations of GD, so let's not do this here. However is good to have an intuition.\n",
    "\n",
    "For illustration purposes, let's think about a function with two parameters. Something like this one:\n",
    "\n",
    "![Gradient Descent](./images/Gradient_descent.png)\n",
    "\n",
    "Gradient descent will try to find the minimum of the function. To do so, we calculate the slope of the function at a certain point, and move towards the direction that makes the function decrease. There are some things to have in mind though.\n",
    "\n",
    "As you can see a function can have one or several _local minimum_. In a local minimum, the slope will be zero and GD will \"think\" it's found the global minimum. To avoid this, we can choose a bigger \"step\" when we move towards the minimum. The \"size\" of the step towards the minimun is what we call the **learning rate**, and it's another adjustable parameter.\n",
    "\n",
    "We need to be careful here: If we choose a too small learning rate, we can get stuck in a local minimum. If we instead choose a too big learning rate, we risk overshooting the global minimum. We need to experiment, and the adecuate learning rate depends on the particular problem and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training process\n",
    "\n",
    "In order for logistic regression to learn, we need to repeat the process descrived before several times. Each one of these times is called an **epoch**. The number of epochs to run depends on the problem and the training data. It is... yes, another tunnable parameter of the algorithm.\n",
    "\n",
    "The set of all tunnable parameters is called **hyperparameters** of the model.\n",
    "\n",
    "Like with the leatning rate, we need to be careful when choosing the number of epochs:If we train too many epochs, we risk **overfitting**. This means that our model will \"memorize\" the training data and will generalize badly when presented new data. \n",
    "\n",
    "If we train too little, it will fail to find any pattern and the prediction accuracy will be very low. This is known as **underfitting**.\n",
    "\n",
    "This is why when training a model you need to set aside a _test dataset_ in order to know the accuracy of your algorithm in unknown data. The test dataset will **never** be used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3.6]",
   "language": "python",
   "name": "conda-env-python3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
